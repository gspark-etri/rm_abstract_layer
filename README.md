# RM Abstract Layer

> **GPU / NPU / CPU ì´ê¸°ì¢… ê°€ì†ê¸°ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” LLM ì¶”ë¡  ì¶”ìƒí™” ë ˆì´ì–´**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ ê°œìš”

RM Abstract LayerëŠ” ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°(GPU, NPU, CPU)ì—ì„œ LLM ì¶”ë¡ ì„ **ì½”ë“œ ìˆ˜ì • ì—†ì´** ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì¶”ìƒí™” ë ˆì´ì–´ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- ğŸ”„ **ë””ë°”ì´ìŠ¤ íˆ¬ëª…ì„±**: ê¸°ì¡´ HuggingFace ì½”ë“œê°€ ê·¸ëŒ€ë¡œ ë™ì‘
- âš¡ **ëŸ°íƒ€ì„ ìŠ¤ìœ„ì¹­**: GPU â†” CPU â†” NPU ì‹¤ì‹œê°„ ì „í™˜
- ğŸš€ **ë‹¤ì¤‘ ì„œë¹™ ì—”ì§„**: vLLM, Triton, TorchServe ì§€ì›
- ğŸ”Œ **í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ ë°±ì—”ë“œ ì‰½ê²Œ ì¶”ê°€
- ğŸŒ **REST API**: OpenAI í˜¸í™˜ API ì„œë²„

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install -e .

# GPU ì§€ì›
pip install -e ".[gpu]"

# ì „ì²´ ì„¤ì¹˜
pip install -e ".[all]"
```

### ì‹œìŠ¤í…œ í™•ì¸

```bash
# ì‹œìŠ¤í…œ ê²€ì¦ (ì‹¤ì œ í…ŒìŠ¤íŠ¸)
python -m rm_abstract.system_validator

# ì„¤ì¹˜ ìƒíƒœ í™•ì¸
python -m rm_abstract.installer
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import rm_abstract
from transformers import AutoModelForCausalLM, AutoTokenizer

# ì´ˆê¸°í™” (ìë™ìœ¼ë¡œ ìµœì  ë””ë°”ì´ìŠ¤ ì„ íƒ)
rm_abstract.init(device="auto")

# ê¸°ì¡´ HuggingFace ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# ì¶”ë¡ 
inputs = tokenizer("Hello, I am", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))
```

### ë””ë°”ì´ìŠ¤ ì „í™˜

```python
import rm_abstract

# GPUë¡œ ì‹œì‘
rm_abstract.init(device="gpu:0")

# CPUë¡œ ì „í™˜
rm_abstract.switch_device("cpu")

# í˜„ì¬ ë””ë°”ì´ìŠ¤ í™•ì¸
info = rm_abstract.get_device_info()
print(f"í˜„ì¬: {info['device_type']}:{info['device_id']}")
```

---

## ğŸ—ï¸ ì§€ì› í™˜ê²½

### í•˜ë“œì›¨ì–´

| ë””ë°”ì´ìŠ¤ | ìƒíƒœ | ë°±ì—”ë“œ |
|----------|------|--------|
| NVIDIA GPU | âœ… ì§€ì› | vLLM |
| CPU | âœ… ì§€ì› | PyTorch |
| Rebellions ATOM NPU | âœ… ì§€ì› | vLLM-RBLN / Optimum-RBLN |
| FuriosaAI NPU | ğŸ”„ ê³„íš | - |

### ì„œë¹™ ì—”ì§„

| ì—”ì§„ | ìƒíƒœ | íŠ¹ì§• | ì‹¤í–‰ ë°©ì‹ |
|------|------|------|----------|
| vLLM | âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ | ê³ ì„±ëŠ¥ LLM ì„œë¹™ | ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ì‹¤í–‰ |
| Triton | âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ | ë‹¤ì¤‘ ëª¨ë¸ ì„œë¹™ | Docker ì»¨í…Œì´ë„ˆ ìë™ ê´€ë¦¬ |
| TorchServe | âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ | PyTorch ë„¤ì´í‹°ë¸Œ | Java ì„œë²„ ìë™ ê´€ë¦¬ |

---

## ğŸ”„ í†µí•© ì„œë¹™ ì¸í„°í˜ì´ìŠ¤

ëª¨ë“  ì„œë¹™ ì—”ì§„ì„ **ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤**ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from rm_abstract.serving import create_serving_engine, ServingConfig, ServingEngineType

# vLLM ì‚¬ìš©
config = ServingConfig(engine=ServingEngineType.VLLM, model_name="gpt2")
with create_serving_engine(config) as engine:
    engine.load_model("gpt2")
    output = engine.infer("Hello, I am")
    print(output)

# Triton ì‚¬ìš© (Docker ìë™ ì‹œì‘/ì¢…ë£Œ)
config = ServingConfig(engine=ServingEngineType.TRITON, port=8000)
with create_serving_engine(config) as engine:
    engine.load_model("gpt2")
    output = engine.infer("Hello, I am")
    print(output)

# TorchServe ì‚¬ìš© (ì„œë²„ ìë™ ì‹œì‘/ì¢…ë£Œ)
config = ServingConfig(engine=ServingEngineType.TORCHSERVE, port=8080)
with create_serving_engine(config) as engine:
    engine.load_model("gpt2")
    output = engine.infer("Hello, I am")
    print(output)
```

### Context Manager ì§€ì›

- `with` ë¸”ë¡ ì§„ì… ì‹œ ì„œë²„ ìë™ ì‹œì‘
- `with` ë¸”ë¡ ì¢…ë£Œ ì‹œ ì„œë²„ ìë™ ì •ë¦¬
- ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ì•ˆì „í•˜ê²Œ ì •ë¦¬

---

## ğŸŒ REST API ì„œë²„

OpenAI í˜¸í™˜ API ì„œë²„ ì œê³µ:

```bash
# ì„œë²„ ì‹œì‘
python -m rm_abstract.api --port 8000

# API ë¬¸ì„œ
open http://localhost:8000/docs
```

```bash
# í…ìŠ¤íŠ¸ ìƒì„±
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt2", "prompt": "Hello", "max_tokens": 50}'

# ì±„íŒ…
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt2", "messages": [{"role": "user", "content": "Hi!"}]}'
```

---

## ğŸ“Š ì‹œìŠ¤í…œ ê²€ì¦

```bash
python -m rm_abstract.system_validator
```

```
======================================================================
  RM Abstract Layer - System Validation
======================================================================

  Testing GPU Available... âœ“
  Testing CPU Inference... âœ“
  Testing vLLM GPU Inference... âœ“
  Testing Device Switching... âœ“
  Testing Triton... âœ“
  Testing TorchServe... âœ“

Summary:
  âœ… Passed:   6
  âŒ Failed:   0
======================================================================
```

---

## ğŸ“š ë¬¸ì„œ

| ë¬¸ì„œ | ì„¤ëª… |
|------|------|
| [INSTALL.md](INSTALL.md) | ìƒì„¸ ì„¤ì¹˜ ê°€ì´ë“œ |
| [QUICKSTART.md](QUICKSTART.md) | ì˜ˆì œ ì¤‘ì‹¬ ë¹ ë¥¸ ì‹œì‘ |
| [ARCHITECTURE.md](ARCHITECTURE.md) | ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ |
| [API.md](API.md) | REST API ë ˆí¼ëŸ°ìŠ¤ |
| [CONTRIBUTING.md](CONTRIBUTING.md) | ê°œë°œ/ê¸°ì—¬ ê°€ì´ë“œ |

---

## ğŸ› ï¸ ì˜ˆì œ

```
examples/
â”œâ”€â”€ basic_usage.py           # ê¸°ë³¸ ì‚¬ìš©ë²•
â”œâ”€â”€ device_switching.py      # ë””ë°”ì´ìŠ¤ ì „í™˜
â”œâ”€â”€ gpu_vllm_usage.py        # GPU/vLLM ì‚¬ìš© ì˜ˆì œ
â”œâ”€â”€ serving_engines_demo.py  # ì„œë¹™ ì—”ì§„ ë¹„êµ
â””â”€â”€ unified_serving_demo.py  # í†µí•© ì„œë¹™ ì¸í„°í˜ì´ìŠ¤ (ìë™ ì„œë²„ ê´€ë¦¬)
```

```bash
# ê¸°ë³¸ ì‚¬ìš©ë²•
python examples/basic_usage.py

# GPU/vLLM ì˜ˆì œ ì‹¤í–‰
python examples/gpu_vllm_usage.py

# í†µí•© ì„œë¹™ ë°ëª¨ (ê¶Œì¥)
python examples/unified_serving_demo.py

# íŠ¹ì • ì—”ì§„ìœ¼ë¡œ ì„œë¹™ ë°ëª¨
python examples/serving_engines_demo.py --engine vllm
python examples/serving_engines_demo.py --engine triton
python examples/serving_engines_demo.py --engine torchserve
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
pytest tests/ -v

# ì½”ì–´ í…ŒìŠ¤íŠ¸ë§Œ
pytest tests/test_core.py -v

# API í…ŒìŠ¤íŠ¸ë§Œ
pytest tests/test_api.py -v
```

---

## ğŸ“¦ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
rm_abstract_layer/
â”œâ”€â”€ src/rm_abstract/
â”‚   â”œâ”€â”€ api/              # REST API ì„œë²„ (OpenAI í˜¸í™˜)
â”‚   â”œâ”€â”€ backends/         # ë°±ì—”ë“œ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ cpu/          # CPU ë°±ì—”ë“œ (PyTorch)
â”‚   â”‚   â”œâ”€â”€ gpu/          # GPU ë°±ì—”ë“œ (vLLM)
â”‚   â”‚   â””â”€â”€ npu/          # NPU ë°±ì—”ë“œ (Rebellions RBLN)
â”‚   â”œâ”€â”€ serving/          # ì„œë¹™ ì—”ì§„ (í†µí•© ì¸í„°í˜ì´ìŠ¤)
â”‚   â”‚   â”œâ”€â”€ vllm_engine.py      # vLLM (ë¼ì´ë¸ŒëŸ¬ë¦¬)
â”‚   â”‚   â”œâ”€â”€ triton_engine.py    # Triton (Docker)
â”‚   â”‚   â””â”€â”€ torchserve_engine.py # TorchServe (Java)
â”‚   â”œâ”€â”€ core/             # ì½”ì–´ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ system_info.py    # ì‹œìŠ¤í…œ ì •ë³´
â”‚   â”œâ”€â”€ system_validator.py # ì‹œìŠ¤í…œ ê²€ì¦
â”‚   â””â”€â”€ installer.py      # ì„¤ì¹˜ í—¬í¼
â”œâ”€â”€ tests/                # pytest í…ŒìŠ¤íŠ¸
â”œâ”€â”€ examples/             # ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements/         # ì˜ì¡´ì„± íŒŒì¼
â”œâ”€â”€ scripts/              # ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ docker/               # Docker ì„¤ì •
    â””â”€â”€ Dockerfile.triton # Triton ì»¤ìŠ¤í…€ ì´ë¯¸ì§€
```

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License

---

## ğŸ¤ ê¸°ì—¬

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! [CONTRIBUTING.md](CONTRIBUTING.md)ë¥¼ ì°¸ì¡°í•´ì£¼ì„¸ìš”.
