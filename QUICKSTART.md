# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

ì˜ˆì œ ì¤‘ì‹¬ìœ¼ë¡œ RM Abstract Layer ì‚¬ìš©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.

---

## ğŸ¯ 5ë¶„ ë§Œì— ì‹œì‘í•˜ê¸°

### 1. ê°€ìƒí™˜ê²½ ì„¤ì •

```bash
# uv ì‚¬ìš© (ê¶Œì¥ - ë¹ ë¦„!)
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc
uv venv .venv && source .venv/bin/activate

# ë˜ëŠ” venv ì‚¬ìš©
python -m venv .venv && source .venv/bin/activate
```

### 2. ì„¤ì¹˜

```bash
# uvë¡œ ì„¤ì¹˜ (ë¹ ë¦„!)
uv pip install -e ".[gpu]"

# ë˜ëŠ” pip ì‚¬ìš©
pip install -e ".[gpu]"
```

### 3. ì‹œìŠ¤í…œ í™•ì¸

```bash
python -m rm_abstract.system_validator --quick
```

### 4. ì²« ë²ˆì§¸ ì˜ˆì œ

```python
import rm_abstract
from transformers import AutoModelForCausalLM, AutoTokenizer

# ì´ˆê¸°í™”
rm_abstract.init(device="auto")

# ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

inputs = tokenizer("Hello, I am", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=30)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸ“š ì˜ˆì œ ëª¨ìŒ

### ì˜ˆì œ 1: GPU/vLLM ì‚¬ìš©

```python
import rm_abstract
from transformers import AutoModelForCausalLM, AutoTokenizer

if __name__ == "__main__":
    # GPU ì´ˆê¸°í™”
    rm_abstract.init(device="gpu:0", verbose=True)

    # ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")

    # í…ìŠ¤íŠ¸ ìƒì„±
    prompt = "The future of AI is"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        temperature=0.7,
        do_sample=True,
    )

    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

> âš ï¸ **ì°¸ê³ **: vLLM ì‚¬ìš© ì‹œ `if __name__ == "__main__":` ê°€ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.

### ì˜ˆì œ 2: GPU â†’ CPU ì „í™˜

```python
import rm_abstract
from transformers import AutoModelForCausalLM, AutoTokenizer

if __name__ == "__main__":
    # GPUë¡œ ì‹œì‘
    rm_abstract.init(device="gpu:0")
    print(f"í˜„ì¬ ë””ë°”ì´ìŠ¤: {rm_abstract.get_device_info()}")

    # CPUë¡œ ì „í™˜
    rm_abstract.switch_device("cpu")
    print(f"ì „í™˜ í›„: {rm_abstract.get_device_info()}")

    # CPUì—ì„œ ì¶”ë¡ 
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")

    inputs = tokenizer("Hello", return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=20)
    print(tokenizer.decode(outputs[0]))
```

### ì˜ˆì œ 3: ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸

```bash
# í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
python -m rm_abstract.system_info
```

```python
# Pythonì—ì„œ ì‹¤í–‰
import rm_abstract

# ì „ì²´ ì‹œìŠ¤í…œ ì •ë³´
rm_abstract.print_system_info()

# ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ
backends = rm_abstract.get_available_backends()
for name, available in backends.items():
    status = "âœ“" if available else "âœ—"
    print(f"  {status} {name}")
```

### ì˜ˆì œ 4: REST API ì„œë²„

**ì„œë²„ ì‹œì‘:**
```bash
python -m rm_abstract.api --port 8000
```

**API í˜¸ì¶œ:**
```bash
# í…ìŠ¤íŠ¸ ìƒì„±
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt2", "prompt": "Hello", "max_tokens": 30}'

# ì±„íŒ…
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt2", "messages": [{"role": "user", "content": "Hi!"}]}'
```

**Python í´ë¼ì´ì–¸íŠ¸:**
```python
import requests

# í…ìŠ¤íŠ¸ ìƒì„±
response = requests.post(
    "http://localhost:8000/v1/completions",
    json={"model": "gpt2", "prompt": "Hello, I am", "max_tokens": 30}
)
print(response.json()["choices"][0]["text"])
```

### ì˜ˆì œ 5: ì„œë¹™ ì—”ì§„ ì‚¬ìš©

```python
from rm_abstract.serving import (
    create_serving_engine,
    ServingConfig,
    ServingEngineType,
    DeviceTarget,
)

if __name__ == "__main__":
    # vLLM ì—”ì§„
    config = ServingConfig(
        engine=ServingEngineType.VLLM,
        device=DeviceTarget.GPU,
    )
    engine = create_serving_engine(config)
    engine.load_model("gpt2")
    output = engine.infer("Hello, I am", max_tokens=30)
    print(output)
```

---

## ğŸ”§ ë””ë°”ì´ìŠ¤ ì˜µì…˜

```python
import rm_abstract

# ìë™ ì„ íƒ (NPU > GPU > CPU)
rm_abstract.init(device="auto")

# íŠ¹ì • GPU
rm_abstract.init(device="gpu:0")
rm_abstract.init(device="gpu:1")

# CPU
rm_abstract.init(device="cpu")

# Rebellions NPU
rm_abstract.init(device="rbln:0")
```

---

## ğŸ“ ì˜ˆì œ íŒŒì¼ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
source .venv/bin/activate

# ì˜ˆì œ ì‹¤í–‰
python examples/gpu_vllm_usage.py
python examples/serving_engines_demo.py
```

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `gpu_vllm_usage.py` | GPU/vLLM ì‚¬ìš© ë° ë””ë°”ì´ìŠ¤ ìŠ¤ìœ„ì¹­ |
| `serving_engines_demo.py` | vLLM, Triton, TorchServe ë¹„êµ |

---

## â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q: ì–´ë–¤ ë””ë°”ì´ìŠ¤ê°€ ì‚¬ìš©ë˜ë‚˜ìš”?

```python
import rm_abstract

rm_abstract.init(device="auto")
info = rm_abstract.get_device_info()
print(f"ë””ë°”ì´ìŠ¤: {info['device_type']}:{info['device_id']}")
```

### Q: GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ìš”

```bash
# ë‹¤ë¥¸ GPU ì‚¬ìš©
CUDA_VISIBLE_DEVICES=1 python script.py

# ë˜ëŠ” CPU ì‚¬ìš©
rm_abstract.init(device="cpu")
```

### Q: ê¸°ì¡´ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ë‚˜ìš”?

ì•„ë‹ˆìš”! `rm_abstract.init()` í•œ ì¤„ë§Œ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤:

```python
import rm_abstract
rm_abstract.init()  # ì´ í•œ ì¤„ë§Œ ì¶”ê°€

# ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("gpt2")
```

### Q: vLLM ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•´ìš”

vLLMì€ `spawn` ë°©ì‹ì˜ ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ì— ë‹¤ìŒì„ ì¶”ê°€í•˜ì„¸ìš”:

```python
if __name__ == "__main__":
    # ì½”ë“œë¥¼ ì—¬ê¸°ì— ì‘ì„±
    main()
```

---

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„

- [INSTALL.md](INSTALL.md) - ìƒì„¸ ì„¤ì¹˜ ê°€ì´ë“œ (ê°€ìƒí™˜ê²½ ìƒì„¸ ì„¤ëª…)
- [API.md](API.md) - REST API ë ˆí¼ëŸ°ìŠ¤
- [ARCHITECTURE.md](ARCHITECTURE.md) - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
