# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

ì˜ˆì œ ì¤‘ì‹¬ìœ¼ë¡œ RM Abstract Layer ì‚¬ìš©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.

---

## ğŸ¯ 5ë¶„ ë§Œì— ì‹œì‘í•˜ê¸°

### 1. ì„¤ì¹˜

```bash
pip install -e ".[gpu]"
```

### 2. ì‹œìŠ¤í…œ í™•ì¸

```bash
python -m rm_abstract.system_validator --quick
```

### 3. ì²« ë²ˆì§¸ ì˜ˆì œ

```python
import rm_abstract
from transformers import AutoModelForCausalLM, AutoTokenizer

# ì´ˆê¸°í™”
rm_abstract.init(device="auto")

# ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

inputs = tokenizer("Hello, I am", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=30)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸ“š ì˜ˆì œ ëª¨ìŒ

### ì˜ˆì œ 1: GPU/vLLM ì‚¬ìš©

```python
import rm_abstract
from transformers import AutoModelForCausalLM, AutoTokenizer

# GPU ì´ˆê¸°í™”
rm_abstract.init(device="gpu:0", verbose=True)

# ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# í…ìŠ¤íŠ¸ ìƒì„±
prompt = "The future of AI is"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    temperature=0.7,
    do_sample=True,
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### ì˜ˆì œ 2: CPUë¡œ ì „í™˜

```python
import rm_abstract
from transformers import AutoModelForCausalLM, AutoTokenizer

# GPUë¡œ ì‹œì‘
rm_abstract.init(device="gpu:0")
print(f"í˜„ì¬ ë””ë°”ì´ìŠ¤: {rm_abstract.get_controller().device_name}")

# CPUë¡œ ì „í™˜
rm_abstract.switch_device("cpu")
print(f"ì „í™˜ í›„: {rm_abstract.get_controller().device_name}")

# CPUì—ì„œ ì¶”ë¡ 
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

inputs = tokenizer("Hello", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0]))
```

### ì˜ˆì œ 3: ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸

```python
import rm_abstract

# ì „ì²´ ì‹œìŠ¤í…œ ì •ë³´
rm_abstract.print_system_info()

# ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
info = rm_abstract.get_system_info()
print(f"GPU ê°œìˆ˜: {len(info.gpus)}")
print(f"NPU ê°œìˆ˜: {len(info.npus)}")

# ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ
backends = rm_abstract.get_available_backends()
for name, available in backends.items():
    status = "âœ“" if available else "âœ—"
    print(f"  {status} {name}")
```

### ì˜ˆì œ 4: REST API ì„œë²„

```python
# ì„œë²„ ì‹œì‘ (ë³„ë„ í„°ë¯¸ë„)
# python -m rm_abstract.api --port 8000

import requests

# í…ìŠ¤íŠ¸ ìƒì„±
response = requests.post(
    "http://localhost:8000/v1/completions",
    json={
        "model": "gpt2",
        "prompt": "Hello, I am",
        "max_tokens": 30,
    }
)
print(response.json()["choices"][0]["text"])

# ì±„íŒ…
response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "gpt2",
        "messages": [
            {"role": "user", "content": "What is AI?"}
        ],
        "max_tokens": 50,
    }
)
print(response.json()["choices"][0]["message"]["content"])
```

### ì˜ˆì œ 5: ì„œë¹™ ì—”ì§„ ì‚¬ìš©

```python
from rm_abstract.serving import (
    create_serving_engine,
    ServingConfig,
    ServingEngineType,
    DeviceTarget,
)

# vLLM ì—”ì§„
config = ServingConfig(
    engine=ServingEngineType.VLLM,
    device=DeviceTarget.GPU,
)
engine = create_serving_engine(config)
engine.load_model("gpt2")
output = engine.infer("Hello, I am", max_tokens=30)
print(output)
```

---

## ğŸ”§ ë””ë°”ì´ìŠ¤ ì˜µì…˜

```python
import rm_abstract

# ìë™ ì„ íƒ (NPU > GPU > CPU)
rm_abstract.init(device="auto")

# íŠ¹ì • GPU
rm_abstract.init(device="gpu:0")
rm_abstract.init(device="gpu:1")

# CPU
rm_abstract.init(device="cpu")

# Rebellions NPU
rm_abstract.init(device="rbln:0")
```

---

## ğŸ“ ì˜ˆì œ íŒŒì¼

```bash
# ì˜ˆì œ ì‹¤í–‰
python examples/gpu_vllm_usage.py
python examples/serving_engines_demo.py
```

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `gpu_vllm_usage.py` | GPU/vLLM ì‚¬ìš© ë° ë””ë°”ì´ìŠ¤ ìŠ¤ìœ„ì¹­ |
| `serving_engines_demo.py` | vLLM, Triton, TorchServe ë¹„êµ |

---

## â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### Q: ì–´ë–¤ ë””ë°”ì´ìŠ¤ê°€ ì‚¬ìš©ë˜ë‚˜ìš”?

```python
import rm_abstract

rm_abstract.init(device="auto")
info = rm_abstract.get_device_info()
print(f"ë””ë°”ì´ìŠ¤: {info['device_type']}:{info['device_id']}")
```

### Q: GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ìš”

```bash
# ë‹¤ë¥¸ GPU ì‚¬ìš©
CUDA_VISIBLE_DEVICES=1 python script.py

# ë˜ëŠ” CPU ì‚¬ìš©
rm_abstract.init(device="cpu")
```

### Q: ê¸°ì¡´ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ë‚˜ìš”?

ì•„ë‹ˆìš”! `rm_abstract.init()` í•œ ì¤„ë§Œ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤:

```python
import rm_abstract
rm_abstract.init()  # ì´ í•œ ì¤„ë§Œ ì¶”ê°€

# ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("gpt2")
```

---

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„

- [INSTALL.md](INSTALL.md) - ìƒì„¸ ì„¤ì¹˜ ê°€ì´ë“œ
- [API.md](API.md) - REST API ë ˆí¼ëŸ°ìŠ¤
- [ARCHITECTURE.md](ARCHITECTURE.md) - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
