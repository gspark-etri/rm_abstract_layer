# Docker Compose for RM Abstract Layer Serving Engines
# 
# Usage:
#   docker-compose up triton      # Start Triton only
#   docker-compose up             # Start all services
#   docker-compose down           # Stop all services

version: '3.8'

services:
  # Triton Inference Server
  triton:
    image: nvcr.io/nvidia/tritonserver:24.01-py3
    container_name: rm_triton
    ports:
      - "8000:8000"   # HTTP
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics
    volumes:
      - ${TRITON_MODEL_REPO:-./models/triton}:/models
    command: tritonserver --model-repository=/models --log-verbose=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Triton with RBLN NPU support (if available)
  triton-rbln:
    image: nvcr.io/nvidia/tritonserver:24.01-py3
    container_name: rm_triton_rbln
    ports:
      - "8010:8000"
      - "8011:8001"
      - "8012:8002"
    volumes:
      - ${TRITON_MODEL_REPO:-./models/triton}:/models
      - /dev:/dev  # For NPU access
    command: tritonserver --model-repository=/models
    privileged: true
    restart: unless-stopped
    profiles:
      - rbln

networks:
  default:
    name: rm_abstract_network

