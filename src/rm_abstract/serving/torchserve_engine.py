"""
TorchServe Serving Engine

PyTorch native model serving using TorchServe.
Supports GPU, CPU, and NPU (RBLN) backends.

Reference:
- https://pytorch.org/serve/
- https://docs.rbln.ai/latest/model_serving/torchserve/
"""

from typing import Any, Dict, List, Optional, Union
from pathlib import Path
import logging
import json
import os
import tempfile

from .base import ServingEngine, ServingEngineType, ServingConfig, DeviceTarget
from .utils import wait_for_server, find_executable, run_command
from ..exceptions import (
    ModelLoadError,
    ConfigurationError,
    ServerStartError,
    PackageNotInstalledError,
)

logger = logging.getLogger(__name__)


class TorchServeEngine(ServingEngine):
    """
    TorchServe Serving Engine
    
    PyTorch-native model serving with:
    - Model versioning
    - A/B testing
    - Metrics and logging
    - RESTful API
    - **Auto server management**
    
    Example:
        # 자동 서버 관리 (context manager)
        with TorchServeEngine(config) as engine:
            engine.load_model("gpt2")
            output = engine.infer("Hello")
        # 서버 자동 종료
        
        # 또는 수동 관리
        engine = TorchServeEngine(config)
        engine.load_model("gpt2")
        engine.start_server()  # 서버 자동 시작
        output = engine.infer("Hello")
        engine.stop_server()   # 서버 종료
    """
    
    def __init__(self, config: ServingConfig):
        super().__init__(config)
        self._model_store: Optional[str] = None
        self._mar_file: Optional[str] = None
        self._loaded_model: Optional[str] = None
    
    @property
    def name(self) -> str:
        return "TorchServe"
    
    @property
    def engine_type(self) -> ServingEngineType:
        return ServingEngineType.TORCHSERVE
    
    @classmethod
    def is_available(cls) -> bool:
        """Check if TorchServe is available"""
        try:
            import torch
            # Check torch-model-archiver
            import subprocess
            result = subprocess.run(
                ["torch-model-archiver", "--help"],
                capture_output=True,
            )
            return result.returncode == 0
        except (ImportError, FileNotFoundError):
            return False
    
    @classmethod
    def supported_devices(cls) -> List[DeviceTarget]:
        """TorchServe supports multiple backends"""
        return [
            DeviceTarget.GPU,
            DeviceTarget.CPU,
            DeviceTarget.NPU_RBLN,
        ]
    
    def setup_model_store(self, store_path: str) -> None:
        """
        Setup TorchServe model store directory
        
        Args:
            store_path: Path to model store
        """
        self._model_store = store_path
        Path(store_path).mkdir(parents=True, exist_ok=True)
        logger.info(f"Model store set to: {store_path}")
    
    def create_handler(self, hf_model: str, handler_path: Path) -> str:
        """
        Create TorchServe handler for LLM
        
        Args:
            hf_model: HuggingFace model name
            handler_path: Path to save handler
            
        Returns:
            Path to handler file
        """
        device_code = ""
        if self.config.device == DeviceTarget.NPU_RBLN:
            device_code = f'''
        # Use RBLN NPU
        from optimum.rbln import RBLNModelForCausalLM
        self.model = RBLNModelForCausalLM.from_pretrained(
            model_name,
            export=True,
            rbln_device_id={self.config.device_id},
        )
'''
        elif self.config.device == DeviceTarget.GPU:
            device_code = f'''
        # Use GPU
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        if torch.cuda.is_available():
            self.model = self.model.cuda({self.config.device_id})
'''
        else:
            device_code = '''
        # Use CPU
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
'''
        
        handler_code = f'''"""
TorchServe Handler for LLM
Auto-generated by rm_abstract
"""

import json
import logging
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from ts.torch_handler.base_handler import BaseHandler

logger = logging.getLogger(__name__)


class LLMHandler(BaseHandler):
    """TorchServe handler for LLM inference"""
    
    def __init__(self):
        super().__init__()
        self.model = None
        self.tokenizer = None
        self.initialized = False
    
    def initialize(self, context):
        """Initialize the model"""
        self.manifest = context.manifest
        properties = context.system_properties
        model_dir = properties.get("model_dir")
        
        model_name = "{hf_model}"
        
        logger.info(f"Loading model: {{model_name}}")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load model based on device
{device_code}
        
        self.model.eval()
        self.initialized = True
        logger.info("Model loaded successfully")
    
    def preprocess(self, requests):
        """Preprocess input requests"""
        inputs = []
        for request in requests:
            data = request.get("data") or request.get("body")
            if isinstance(data, (bytes, bytearray)):
                data = data.decode("utf-8")
            if isinstance(data, str):
                try:
                    data = json.loads(data)
                except json.JSONDecodeError:
                    data = {{"text": data}}
            
            text = data.get("text", data.get("prompt", ""))
            inputs.append(text)
        
        return inputs
    
    def inference(self, inputs):
        """Run inference"""
        results = []
        
        for text in inputs:
            # Tokenize
            encoded = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length={self.config.max_seq_len},
            )
            
            # Move to device if needed
            if hasattr(self.model, "device"):
                encoded = {{k: v.to(self.model.device) for k, v in encoded.items()}}
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **encoded,
                    max_new_tokens={self.config.extra_options.get("max_new_tokens", 256)},
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.pad_token_id,
                )
            
            # Decode
            generated_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True,
            )
            results.append(generated_text)
        
        return results
    
    def postprocess(self, outputs):
        """Postprocess outputs"""
        return [{{"generated_text": text}} for text in outputs]
'''
        
        handler_path.write_text(handler_code)
        return str(handler_path)
    
    def create_model_archive(self, model_name_or_path: str, **kwargs) -> str:
        """
        Create TorchServe Model Archive (.mar file)
        
        Args:
            model_name_or_path: HuggingFace model name or path
            **kwargs: Additional options
            
        Returns:
            Path to .mar file
        """
        import subprocess
        
        if self._model_store is None:
            self.setup_model_store(
                os.path.expanduser("~/.rm_abstract/torchserve_models")
            )
        
        model_name = kwargs.get("archive_name", self.config.model_name or "llm_model")
        
        # Create temporary directory for handler
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir = Path(tmpdir)
            
            # Create handler
            handler_path = tmpdir / "handler.py"
            self.create_handler(model_name_or_path, handler_path)
            
            # Create model archive
            mar_path = Path(self._model_store) / f"{model_name}.mar"
            
            cmd = [
                "torch-model-archiver",
                "--model-name", model_name,
                "--version", "1.0",
                "--handler", str(handler_path),
                "--export-path", self._model_store,
                "--force",
            ]
            
            logger.info(f"Creating model archive: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"Failed to create archive: {result.stderr}")
                raise ModelLoadError(model_name, reason=f"torch-model-archiver failed: {result.stderr}")
            
            self._mar_file = str(mar_path)
            logger.info(f"Created model archive: {mar_path}")
            
            return self._mar_file
    
    def load_model(self, model_name_or_path: str, **kwargs) -> Any:
        """
        Load model (create archive if needed)
        
        Args:
            model_name_or_path: Model name or path
            **kwargs: Additional options
        """
        return self.create_model_archive(model_name_or_path, **kwargs)
    
    def create_config(self) -> str:
        """
        Create TorchServe config.properties
        
        Returns:
            Path to config file
        """
        if self._model_store is None:
            raise ConfigurationError("Model store not set. Call setup_model_store() first.")
        
        config_content = f'''inference_address=http://{self.config.host}:{self.config.port}
management_address=http://{self.config.host}:{self.config.port + 1}
metrics_address=http://{self.config.host}:{self.config.port + 2}
model_store={self._model_store}
load_models=all
'''
        
        # Add GPU configuration
        if self.config.device == DeviceTarget.GPU:
            config_content += f'''
number_of_gpu={1}
default_workers_per_model=1
'''
        
        # Add RBLN configuration
        if self.config.device == DeviceTarget.NPU_RBLN:
            config_content += f'''
# RBLN NPU Configuration
rbln_device_id={self.config.device_id}
'''
        
        config_path = Path(self._model_store) / "config.properties"
        config_path.write_text(config_content)
        
        return str(config_path)
    
    def start_server(self) -> None:
        """Start TorchServe server (자동 관리)"""
        if self._is_running:
            logger.warning("Server already running")
            return
        
        if self._model_store is None:
            self.setup_model_store(
                os.path.expanduser("~/.rm_abstract/torchserve_models")
            )
        
        import subprocess
        
        # Check Java (required for TorchServe)
        java_cmd = find_executable("java")
        if java_cmd is None:
            raise PackageNotInstalledError(
                package="Java 11+",
                install_cmd="sudo apt install openjdk-11-jdk"
            )
        
        # Check if torchserve is available
        torchserve_cmd = find_executable("torchserve")
        if torchserve_cmd is None:
            raise PackageNotInstalledError(
                package="torchserve",
                install_cmd="pip install torchserve torch-model-archiver"
            )
        
        # Stop existing server first
        subprocess.run([torchserve_cmd, "--stop"], capture_output=True)
        
        # Create config
        config_path = self.create_config()
        
        # Start server
        cmd = [
            torchserve_cmd,
            "--start",
            "--model-store", self._model_store,
            "--ts-config", config_path,
            "--ncs",  # No config snapshot
        ]
        
        logger.info(f"Starting TorchServe: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"Failed to start TorchServe: {result.stderr}")
            raise ServerStartError("TorchServe", reason=result.stderr)
        
        self._is_running = True
        self._wait_for_server()
        logger.info(f"TorchServe started on port {self.config.port}")
    
    def _wait_for_server(self, timeout: int = 60) -> None:
        """서버가 준비될 때까지 대기"""
        url = f"http://localhost:{self.config.port}/ping"
        if not wait_for_server(url, timeout=timeout):
            logger.warning("TorchServe server may not be fully ready yet")
    
    def stop_server(self) -> None:
        """Stop TorchServe server"""
        torchserve_cmd = find_executable("torchserve")
        if torchserve_cmd:
            run_command([torchserve_cmd, "--stop"])
        
        self._is_running = False
        logger.info("TorchServe stopped")
    
    def infer(self, inputs: Union[str, List[str]], **kwargs) -> Any:
        """
        Run inference via TorchServe REST API
        
        Args:
            inputs: Text prompt or list of prompts
            **kwargs: Additional options
        """
        import requests
        
        model_name = kwargs.get("model_name", self.config.model_name or "llm_model")
        url = f"http://{self.config.host}:{self.config.port}/predictions/{model_name}"
        
        if isinstance(inputs, str):
            inputs = [inputs]
        
        results = []
        for text in inputs:
            response = requests.post(
                url,
                json={"text": text},
            )
            response.raise_for_status()
            result = response.json()
            
            if isinstance(result, list):
                results.extend([r.get("generated_text", "") for r in result])
            else:
                results.append(result.get("generated_text", ""))
        
        return results[0] if len(results) == 1 else results
    
    def get_model_status(self) -> Dict[str, Any]:
        """Get model status from TorchServe"""
        import requests
        
        try:
            url = f"http://{self.config.host}:{self.config.port + 1}/models"
            response = requests.get(url)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            return {"error": str(e)}

